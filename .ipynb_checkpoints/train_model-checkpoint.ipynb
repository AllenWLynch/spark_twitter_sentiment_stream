{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import ml\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import config\n",
    "import os\n",
    "from itertools import product\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from custom_transformer import EmojiExtractor\n",
    "import importlib\n",
    "import string\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Train SVM\")\\\n",
    "    .config(\"spark.executor.memory\",\"4g\")\\\n",
    "    .config(\"spark.driver.memory\",\"5g\")\\\n",
    "    .master('local[24]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = ['label','text','rand']\n",
    "field_types = [T.IntegerType(), T.StringType(), T.FloatType()]\n",
    "\n",
    "training_schema = T.StructType([\n",
    "    T.StructField(field_name, field_type, True) for field_name, field_type in zip(field_names, field_types)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.read.load(os.path.join(config.DATA_DIR, 'emojified_train.csv'), format = 'csv', sep = ',', \n",
    "                            schema = training_schema, header = True).drop('rand')\n",
    "\n",
    "train_data = train_data.filter(train_data['label'] <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_words.txt', 'r') as f:\n",
    "    bigram_words = [line.strip() for line in f.readlines()]\n",
    "\n",
    "tokenize_str = r'((?:{})\\s)?([\\w\\']+|\\$[\\d\\.]+|\\S+)'.format('|'.join(bigram_words))\n",
    "all_stopwords = ml.feature.StopWordsRemover.loadDefaultStopWords('english')\n",
    "all_stopwords.extend(['http','https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_extractor = EmojiExtractor(inputCol = 'text', outputCol = 'emojis')\n",
    "\n",
    "sql_filter = ml.feature.SQLTransformer(statement = \n",
    "        \"SELECT *, REGEXP_REPLACE(text, {}, \\\"\\\") AS cleaned_text FROM __THIS__\"\\\n",
    "        .format(\"\\\"\" + config.EMOJI_PATTERN[:-1] + config.REMOVE_PATTERN +\"\\\"\"))\n",
    "\n",
    "tokenizer = ml.feature.RegexTokenizer(inputCol = 'cleaned_text', \n",
    "                    outputCol = 'tokenized', pattern = tokenize_str, gaps = False)\n",
    "#tokenizer = ml.feature.Tokenizer(inputCol = 'cleaned_text', outputCol = 'tokenized')\n",
    "\n",
    "stopword_remover = ml.feature.StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                                               outputCol='filtered', stopWords=all_stopwords)\n",
    "\n",
    "sql_assembler = ml.feature.SQLTransformer(statement = \n",
    "        \"SELECT *, CONCAT(filtered, emojis) AS bag FROM __THIS__\")\n",
    "\n",
    "counter = ml.feature.CountVectorizer(minDF = 20, maxDF = 75000, vocabSize = 50000, \n",
    "                        inputCol = 'bag', outputCol = 'counts')\n",
    "\n",
    "normalizer = ml.feature.Normalizer(p = 1.0, inputCol = counter.getOutputCol(), outputCol = 'tf_normalized')\n",
    "\n",
    "df_normalizer = ml.feature.IDF(inputCol=normalizer.getOutputCol(), outputCol='features')\n",
    "\n",
    "preprocessing_pipe = ml.Pipeline(stages=[\n",
    "    emoji_extractor, sql_filter, tokenizer, stopword_remover, \n",
    "    sql_assembler, counter, normalizer, df_normalizer\n",
    "]).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|792059|\n",
      "|    0|792012|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1584071, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_data = preprocessing_pipe.transform(train_data)\n",
    "featurized_data = featurized_data.select('features','label')\n",
    "featurized_data.persist()\n",
    "\n",
    "featurized_data.count(), featurized_data.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresser = ml.classification.LogisticRegression(maxIter = 100, featuresCol='features', labelCol= 'label')\n",
    "\n",
    "paramGrid = ml.tuning.ParamGridBuilder().addGrid(regresser.regParam, [0.1,0.01,0.001])\\\n",
    "    .addGrid(regresser.elasticNetParam, [1.0, 0.5, 0.0]).build()\n",
    "evaluator = ml.evaluation.BinaryClassificationEvaluator()\n",
    "tvs = ml.tuning.TrainValidationSplit(estimator=regresser, estimatorParamMaps=paramGrid, evaluator=evaluator, trainRatio=0.95)\n",
    "trained_regressor = tvs.fit(featurized_data).bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spark.read.load(os.path.join(config.DATA_DIR, 'emojified_test.csv'), format = 'csv', sep = ',', \n",
    "                            schema = training_schema, header = True).drop('rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853303882281052"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ml.PipelineModel(stages = [preprocessing_pipe, trained_regressor])\n",
    "\n",
    "predictions = pipeline.transform(test_data)\n",
    "\n",
    "evaluator = ml.evaluation.BinaryClassificationEvaluator(labelCol='label', rawPredictionCol ='rawPrediction')\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save('LR_with_emoji_pipeline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkapp",
   "language": "python",
   "name": "sparkapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
